{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment_9_Two_Layer_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryamaan23/ML-TOOLBOX/blob/master/Copy_of_Assignment_9_Two_Layer_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXmQGNBWwzS1",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "- In this assignment, you will build a two layer neural network for classification from scratch using only numpy.\n",
        "- Please refer to videos on Backpropagation and one reference material shared in additional resources for the understanding required to solve this assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCZeHgK-xAy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Some functions required for testing \"\"\"\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(D, H, C):\n",
        "  il = keras.layers.Input(shape=(D,))\n",
        "  hl = keras.layers.Dense(H, activation = 'relu')(il)\n",
        "  ol = keras.layers.Dense(C, activation = 'softmax')(hl)\n",
        "  model = keras.models.Model(inputs = [il], outputs = [ol])\n",
        "\n",
        "  rng = np.random.RandomState(2020)\n",
        "  model.layers[1].set_weights([rng.rand(D * H).reshape(D, H), rng.rand(H, )])\n",
        "  model.layers[2].set_weights([rng.rand(H * C).reshape(H, C), rng.rand(C, )])\n",
        "  return model\n",
        "\n",
        "def create_inputs(N, D):\n",
        "  rng = np.random.RandomState(2020)\n",
        "  return rng.rand(N * D).reshape(N, D)\n",
        "\n",
        "def set_weights_from_model(tln, test_net):\n",
        "  tln.params['W1'] = test_net.layers[1].get_weights()[0]\n",
        "  tln.params['b1'] = test_net.layers[1].get_weights()[1]\n",
        "  tln.params['W2'] = test_net.layers[2].get_weights()[0]\n",
        "  tln.params['b2'] = test_net.layers[2].get_weights()[1]\n",
        "  return tln\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9OoL62wOux9t",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network. The net has an input dimension of\n",
        "    D, a hidden layer dimension of H, and performs classification over C classes.\n",
        "    We train the network with a softmax loss function and L2 regularization on the\n",
        "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
        "    connected layer.\n",
        "\n",
        "    In other words, the network has the following architecture:\n",
        "\n",
        "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "    The outputs of the second fully-connected layer are the scores for each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
        "        \"\"\"\n",
        "        Initialize the model. \n",
        "        Weights are initialized to small random values and\n",
        "        biases are initialized to zero. \n",
        "        Weights and biases are stored in the\n",
        "        variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "\n",
        "        Inputs:\n",
        "        - input_size: The dimension N of the input data.\n",
        "        - hidden_size: The number of neurons H in the hidden layer.\n",
        "        - output_size: The number of classes C.\n",
        "        \"\"\"\n",
        "\n",
        "        ### Write your code here\n",
        "        self.params = {}\n",
        "        D = input_size\n",
        "        H = hidden_size\n",
        "        C = output_size\n",
        "        self.params['W1'] = np.random.normal(0, std, D*H).reshape(D, H)\n",
        "        self.params['b1'] = np.zeros(H)\n",
        "        self.params['W2'] = np.random.normal(0, std, H*C).reshape(H, C)\n",
        "        self.params['b2'] = np.zeros(C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ScjBCeTwzS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06c80b90-5a95-4ad5-a230-e5ff9df94cda"
      },
      "source": [
        "\"\"\" Test Cases for Initialization\"\"\"\n",
        "tln = TwoLayerNet(2, 3, 2)\n",
        "assert tln.params['W1'].shape == (2, 3)\n",
        "assert tln.params['b1'].shape == (3, )\n",
        "assert tln.params['W2'].shape == (3, 2)\n",
        "assert tln.params['b2'].shape == (2, )\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kh8LRCT9voz1",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "\n",
        "    def forward(self, X):\n",
        "      \"\"\"\n",
        "      Compute the output of a full forward pass of the network.\n",
        "      \n",
        "      First apply weights W1 and biases b1 on inputs and then apply relu non-linearity.\n",
        "      Then apply weights W2 and biases b2 on hidden layer values and then apply softmax non-linearity to get the output\n",
        "      \n",
        "      Inputs:\n",
        "      - X : Input data of shape (N, D). Each X[i] is a training sample\n",
        "      \n",
        "      Outputs:\n",
        "      - y_out : numpy array with Outputs of shape (N, C)\n",
        "      \n",
        "      \"\"\"\n",
        "      # z is value of node before applying activation function\n",
        "      # a is value of node after applying activation function\n",
        "      ### Write your code here\n",
        "      z1 = X@self.params['W1'] + self.params['b1']\n",
        "      a1 = np.maximum(z1, 0)\n",
        "      z2 = a1@self.params['W2'] + self.params['b2']\n",
        "      a2_num = np.e**(z2)\n",
        "      a2_denom = np.sum(a2_num, axis = 1)\n",
        "      a2 = np.zeros(a2_num.shape)\n",
        "      for i in range(z2.shape[0]):\n",
        "        a2[i, :] = a2_num[i, :] / a2_denom[i]\n",
        "      y_out = a2\n",
        "\n",
        "      \n",
        "      \n",
        "      return y_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJeXhkRZwzTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d677a68-8a0a-4bf5-e2f9-f4a882f28e57"
      },
      "source": [
        "\"\"\"Test Cases for Forward pass\"\"\"\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "test_net = create_model(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, test_net)\n",
        "X = create_inputs(4, 2)\n",
        "y_forward = tln.forward(X)\n",
        "assert y_forward.shape == (4, 2)\n",
        "assert np.all(np.isclose(y_forward, test_net.predict(X), atol = 0.0001))\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbOelhQCwzTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this two-layer network to predict labels for\n",
        "        data points. For each data point we predict scores for each of the C\n",
        "        classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
        "          classify.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
        "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
        "          to have class c, where 0 <= c < C.\n",
        "        \"\"\"\n",
        "        y_pred = None\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Implement this function; it should be VERY simple!                #\n",
        "        ###########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        ### Write your code here\n",
        "        y_pred = None\n",
        "        y_out = self.forward(X)\n",
        "        y_pred = np.argmax(y_out, axis = 1)\n",
        "\n",
        "\n",
        "        \n",
        "        return y_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAJuwv5bwzTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46a2bee4-77a2-451d-f87d-8e43e509026a"
      },
      "source": [
        "\"\"\" Test Cases for predict\"\"\"\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "test_net = create_model(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, test_net)\n",
        "X = create_inputs(4, 2)\n",
        "y_pred = tln.predict(X)\n",
        "test_pred = np.argmax(test_net.predict(X), axis = 1)\n",
        "assert np.all(np.isclose(y_pred, test_pred, atol = 0.01))\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabhV02TwzTO",
        "colab_type": "text"
      },
      "source": [
        "#### Loss\n",
        "Note: <br>\n",
        "$L = -\\sum{t_i \\log{p_i}}$ <br>\n",
        "where $p_i$ is probability score predicted by model. <br>\n",
        "$t_i = 1$ for the true class $i$ and $t_i = 0$ for all other classes for a particular sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mO3-DSCwzTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):    \n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute the loss and gradients for a two layer fully connected neural\n",
        "        network.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "          an integer in the range 0 <= y[i] < C.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
        "        the score for class c on input X[i].\n",
        "\n",
        "        If y is not None, instead return a tuple of:\n",
        "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "          samples. (This is the mean loss over N samples)\n",
        "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "          with respect to the loss function; has the same keys as self.params.\n",
        "        \"\"\"\n",
        "\n",
        "        #############################################################################\n",
        "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
        "        # Store the result in the scores variable, which should be an array of      #\n",
        "        # shape (N, C).                                                             #\n",
        "        #############################################################################\n",
        "        \n",
        "        \n",
        "        \n",
        "        ## Write your code here\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        N, D = X.shape\n",
        "        H, C = W2.shape\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        # # Compute the loss\n",
        "        scores = None\n",
        "\n",
        "        #############################################################################\n",
        "        # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
        "        # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
        "        # in the variable loss, which should be a scalar. Use the Categorical       #\n",
        "        # Cross Entropy loss.                                                       #\n",
        "        #############################################################################\n",
        "      \n",
        "        ### Write your code here\n",
        "        z1 = X@self.params['W1'] + self.params['b1']\n",
        "        a1 = np.maximum(z1, 0)\n",
        "        z2 = a1@self.params['W2'] + self.params['b2']\n",
        "        a2_num = np.e**(z2)\n",
        "        a2_denom = np.sum(a2_num, axis = 1)\n",
        "        a2 = np.zeros(a2_num.shape)\n",
        "        for i in range(z2.shape[0]):\n",
        "          a2[i, :] = a2_num[i, :] / a2_denom[i]\n",
        "        y_out = a2\n",
        "        scores = y_out\n",
        "        # # Compute the loss\n",
        "        loss = None\n",
        "        loss_samples = np.zeros(N)\n",
        "        for i in range(N):\n",
        "          loss_samples[i] = -np.log(scores[i, y[i]])\n",
        "        loss = np.mean(loss_samples)\n",
        "        \n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        grads = {}\n",
        "\n",
        "        ## shapes -- dz1 = (N, H), X = (N, D), W1 = (D, H)\n",
        "        ## shapes -- a1 = (N, H), dz2 = (N, C), dW2 = (H, C)\n",
        "        #############################################################################\n",
        "        # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
        "        # and biases. Store the results in the grads dictionary. For example,       #\n",
        "        # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
        "        #############################################################################\n",
        "\n",
        "        ### Write your code here\n",
        "        # Backward pass: compute gradients\n",
        "        grads = {}\n",
        "\n",
        "        da2 = np.zeros((N, C))\n",
        "        for i in range(N):\n",
        "          da2[i, y[i]] = -1/a2[i, y[i]]\n",
        "        \n",
        "\n",
        "        da2_num = np.zeros((N, C))\n",
        "        da2_denom = np.zeros(N)\n",
        "        for i in range(N):\n",
        "          da2_num[i] = (1/a2_denom[i])*da2[i,: ]\n",
        "          da2_denom[i] = np.sum((-a2_num[i, :]/a2_denom[i]**2)*da2[i, :])\n",
        "\n",
        "        for i in range(N):\n",
        "          da2_num[i] += da2_denom[i]\n",
        "        \n",
        "        dz2 = a2_num * da2_num\n",
        "        da1 = dz2@self.params['W2'].T\n",
        "        dW2 = a1.T @ dz2\n",
        "        db2 = np.sum(dz2, axis = 0)\n",
        "        \n",
        "        \n",
        "        drelu = np.vectorize(lambda x: 1 if x>0 else 0)\n",
        "        dz1 = drelu(a1) * da1\n",
        "        dW1 = X.T @ dz1\n",
        "        db1 = np.sum(dz1, axis=0)\n",
        "\n",
        "        grads['W1'] = dW1/N\n",
        "        grads['b1'] = db1/N\n",
        "        grads['W2'] = dW2/N\n",
        "        grads['b2'] = db2/N\n",
        "\n",
        "        return loss, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn1tAYrpwzTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "362b8456-9b4d-4b00-9ce3-9178177f0343"
      },
      "source": [
        "\"\"\" Tests for loss and gradient computation \"\"\"\n",
        "### First compute loss and gradients using keras\n",
        "model = create_model(2, 4, 2)\n",
        "X = create_inputs(4, 2)\n",
        "y = np.array([0, 1, 1, 0])\n",
        "y_onehot = keras.utils.to_categorical(y, 2)\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "batch_size = 4\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X, y_onehot))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_out = model(x_batch_train, training = True)\n",
        "\n",
        "      ## Compute loss value for this minibatch\n",
        "      loss_value = loss_fn(y_batch_train, y_out)\n",
        "    \n",
        "    grads_model = {}\n",
        "    grads_model['W1'], grads_model['b1'], grads_model['W2'], grads_model['b2'] = [dw.numpy() for dw in tape.gradient(loss_value, model.trainable_weights)]\n",
        "\n",
        "### Compute loss and gradients using TwoLayerNet\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, model)\n",
        "loss, grads_tln = tln.loss(X, y)\n",
        "\n",
        "#### Now match\n",
        "## Loss should be correctly computed\n",
        "assert np.isclose(loss, loss_value.numpy(), atol = 0.0001)\n",
        "\n",
        "## Gradients should be correctly computed\n",
        "print(grads_tln['W1'], grads_model['W1'])\n",
        "assert np.all(np.isclose(grads_tln['W1'], grads_model['W1'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['b1'], grads_model['b1'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['W2'], grads_model['W2'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['b2'], grads_model['b2'], atol = 0.0001))\n",
        "\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.0159359  -0.01363542 -0.01441477 -0.00818676]\n",
            " [-0.00592148  0.00506667  0.00535626  0.00304205]] [[ 0.01593591 -0.01363542 -0.01441477 -0.00818677]\n",
            " [-0.00592148  0.00506666  0.00535626  0.00304205]]\n",
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANJB4KJni0ME",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyIYI5btwzTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "    def train(self, X, y, X_val, y_val,\n",
        "              learning_rate=1e-3, num_iters=100,\n",
        "              batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this neural network using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving training data.\n",
        "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
        "          X[i] has label c, where 0 <= c < C.\n",
        "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
        "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
        "        - learning_rate: Scalar giving learning rate for optimization.\n",
        "        - num_iters: Number of steps to take when optimizing.\n",
        "        - batch_size: Number of training examples to use per step.\n",
        "        \"\"\"\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "        # Use SGD to optimize the parameters in self.model\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "        \n",
        "        ## Create a copy of X and shuffle it\n",
        "        shuffled_indices = np.arange(X.shape[0])\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        dataset_size = X.shape[0]\n",
        "        X_shuffled = X[shuffled_indices]\n",
        "        y_shuffled = y[shuffled_indices]\n",
        "\n",
        "        for it in range(num_iters):\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO: Create a random minibatch of training data and labels, storing  #\n",
        "            # them in X_batch and y_batch respectively.                             #\n",
        "            #########################################################################\n",
        "\n",
        "            \n",
        "            ### Write your code here\n",
        "\n",
        "            start = (num_iters * batch_size)%dataset_size\n",
        "            X_batch = X_shuffled[start: start + batch_size]\n",
        "            y_batch = y_shuffled[start: start + batch_size]\n",
        "\n",
        "            \n",
        "            \n",
        "            # Compute loss and gradients using the current minibatch\n",
        "            loss, grads = self.loss(X_batch, y=y_batch)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO: Use the gradients in the grads dictionary to update the         #\n",
        "            # parameters of the network (stored in the dictionary self.params)      #\n",
        "            # using stochastic gradient descent. You'll need to use the gradients   #\n",
        "            # stored in the grads dictionary defined above.                         #\n",
        "            #########################################################################\n",
        "\n",
        "            \n",
        "            ### Write your code here\n",
        "            self.params['W1'] -= learning_rate * grads['W1']\n",
        "            self.params['W2'] -= learning_rate * grads['W2']\n",
        "            self.params['b1'] -= learning_rate * grads['b1']\n",
        "            self.params['b2'] -= learning_rate * grads['b2']\n",
        "               \n",
        "\n",
        "            # Every epoch, check train and val accuracy\n",
        "            if it % iterations_per_epoch == 0:\n",
        "                # Check accuracy\n",
        "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "                val_acc = (self.predict(X_val) == y_val).mean()\n",
        "                train_acc_history.append(train_acc)\n",
        "                val_acc_history.append(val_acc)\n",
        "\n",
        "                # Decay learning rate\n",
        "                # learning_rate *= learning_rate_decay\n",
        "\n",
        "        return {\n",
        "          'loss_history': loss_history,\n",
        "          'train_acc_history': train_acc_history,\n",
        "          'val_acc_history': val_acc_history,\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYEfUA1nmCju",
        "colab_type": "text"
      },
      "source": [
        "### Using these networks on datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsACefMSmFX0",
        "colab_type": "text"
      },
      "source": [
        "### XOR\n",
        "Use TwoLayerNet to train the XOR function discussed in the class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeUsLKYEwzTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "f49b08b4-ab27-47da-d104-46d809b2905d"
      },
      "source": [
        "### Write your code here\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "xor_net = TwoLayerNet(2, 5, 2)\n",
        "h = xor_net.train(X_xor, y_xor, X_xor, y_xor, batch_size = 4, num_iters = 20000, learning_rate = 0.01)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(h['loss_history'])\n",
        "print(h['loss_history'][-10:])\n",
        "xor_net.predict(X_xor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.009105361504837924, 0.009104982900249168, 0.009103419154620372, 0.009101856248674512, 0.009100294181417405, 0.009099813975385437, 0.009100702147715337, 0.009099898847350915, 0.009098336630647718, 0.009096775252579535]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndDm167ybJ6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8b9ac4d4-d328-472c-a3c9-35109fc8f2f4"
      },
      "source": [
        "plt.plot(h['val_acc_history'])\n",
        "h['val_acc_history'][-10:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3D4GjcImFbW",
        "colab_type": "text"
      },
      "source": [
        "### Iris\n",
        "Use TwoLayerNet to train the iris dataset. Choose 120 samples randomly for training and the rest for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSjLcJT1mX-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "74fb4ec2-a7aa-448a-a265-12d79dcc6e7c"
      },
      "source": [
        "### Write your code here\n",
        "### Write your code here\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "random_indices = np.arange(150)\n",
        "np.random.shuffle(random_indices)\n",
        "X_iris = iris['data'][random_indices[:120]]\n",
        "y_iris = iris['target'][random_indices[:120]]\n",
        "X_iris_val = iris['data'][random_indices[120:]]\n",
        "y_iris_val = iris['target'][random_indices[120:]]\n",
        "iris_net = TwoLayerNet(4, 64, 3)\n",
        "h = iris_net.train(X_iris, y_iris, X_iris_val, y_iris_val, num_iters = 1000, batch_size = 20, learning_rate = 0.05)\n",
        "plt.plot(h['loss_history'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1764ce8d68>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eApJwFDLbdgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "887615df-b244-4f0a-9dce-ba7471f5bf9d"
      },
      "source": [
        "plt.plot(h['val_acc_history'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1764cdc7f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbFD9EXlmaDI",
        "colab_type": "text"
      },
      "source": [
        "### Advanced\n",
        "Add weight regularization to the loss and rewrite backprop part of TwoLayerNet. <br>\n",
        "Train using some datasets and see if regularized network performs better than its older counterpart.\n",
        "<br>\n",
        "The expression for loss with regularization is as follows - <br>\n",
        "$L = -\\sum{t_i \\log{p_i}} + \\lambda(|w_1|^2 + |w_2|^2)$ <br>\n",
        "$\\lambda$ is a tunable hyper-parameter  denoting strength of regularization. <br>\n",
        "If it is too high, network will struggle to fit, and if it is too low, network will overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBAJhyB9oB8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}